{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import yaml\n",
    "from box import ConfigBox\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set web serice URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local testing\n",
    "API_URL = \"http://0.0.0.0:8080\" \n",
    "# testing deployed service\n",
    "# API_URL = \"https://<YOUR_APP_NAME>.fly.dev\" \n",
    "\n",
    "API_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load feature columns and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(params_path):\n",
    "    with open(params_path, \"r\") as f:\n",
    "        params = yaml.safe_load(f)\n",
    "        params = ConfigBox(params)\n",
    "    return params\n",
    "\n",
    "\n",
    "proj_path = Path(os.getcwd()).parent.absolute()\n",
    "params = load_params(proj_path/'params.yaml')\n",
    "feat_cols = params.base.feat_cols\n",
    "targ_col = params.base.targ_col\n",
    "feat_cols, targ_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(proj_path/'data'/'raw'/f'Churn_Modelling_{country}.csv') for country in ['France', 'Spain']])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `/predict` endpoint by sending one sample request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_obj = {\n",
    "    \"data\": [\n",
    "            {\n",
    "      \"CreditScore\": 619,\n",
    "      \"Age\": 42,\n",
    "      \"Tenure\": 2,\n",
    "      \"Balance\": 0,\n",
    "      \"NumOfProducts\": 1,\n",
    "      \"HasCrCard\": 1,\n",
    "      \"IsActiveMember\": 1,\n",
    "      \"EstimatedSalary\": 101348.88\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "x = requests.post(API_URL + '/predict', json = my_obj)\n",
    "if x.ok:\n",
    "    probs = x.json()\n",
    "else:\n",
    "    x.raise_for_status()\n",
    "\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a functions for calling `/predict` endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(obj):\n",
    "    x = requests.post(API_URL + '/predict', json = obj)\n",
    "    if x.ok:\n",
    "        probs = x.json()\n",
    "    else:\n",
    "        x.raise_for_status()\n",
    "    return probs\n",
    "\n",
    "get_prob(my_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check what would f1-scores look like if we send the data from the same geographies that the model was trained on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def send_sample_requests(df):\n",
    "    f1_score_list = []\n",
    "    for _ in tqdm(range(50)):\n",
    "        df_sample = df.sample(n=60)\n",
    "        y_true = df_sample[targ_col].values\n",
    "        obj = {\"data\": df_sample[feat_cols].to_dict('records')}\n",
    "        probs = get_prob(obj)\n",
    "        y_pred = np.array([prob < 0.5 for prob in probs]).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        f1_score_list.append(f1)\n",
    "        time.sleep(1)\n",
    "    return f1_score_list\n",
    "        \n",
    "f1_score_list = send_sample_requests(df)\n",
    "f1_score_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What would f1-scores look like for input data from new geography?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_germany =  pd.read_csv(proj_path/'data'/'more_data'/'Churn_Modelling_Germany.csv')\n",
    "time.sleep(20) # this is to create a pause between requests. Later it'll be easier to distinguish the two types if requests\n",
    "f1_score_list = send_sample_requests(df_germany)\n",
    "f1_score_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... as expected, the values are much lower.\n",
    "### But, typically, we can't compute model metrics on production data right away \n",
    "### because ground truth labels might not be available until much-much later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All we can do is try and look at \"proxy\" metrics that measure statistical differences (distances) between train data and production data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = requests.get(API_URL + '/drift_data')\n",
    "if x.ok:\n",
    "    data = x.json()\n",
    "else:\n",
    "    x.raise_for_status()\n",
    "\n",
    "df_p_vals = pd.DataFrame(json.loads(data))\n",
    "df_p_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Values below the threshold (e.g. 0.05) indicate data drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly_express as px\n",
    "\n",
    "fig = px.line(df_p_vals, x='time', y=feat_cols)\n",
    "fig.add_hline(y=0.05, line_color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fecc1bb5abc8edb62267002c554af8f7f0cf56fe59860ab4d159f72168707bfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
